{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64304648-26a5-4890-82d4-e5e60dfebb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libEGL warning: DRI2: failed to create dri screen\n",
      "libEGL warning: Not allowed to force software rendering when API explicitly selects a hardware device.\n",
      "libEGL warning: DRI2: failed to create dri screen\n",
      "Unable to initialize EGL\n",
      "libEGL warning: DRI2: failed to create dri screen\n",
      "libEGL warning: Not allowed to force software rendering when API explicitly selects a hardware device.\n",
      "libEGL warning: DRI2: failed to create dri screen\n",
      "Unable to initialize EGL\n"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "import sys\n",
    "from __future__ import division\n",
    "\n",
    "import gin\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import functional as T\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.aae.models import AugmentedAutoEncoder\n",
    "from src.aae.Visualizations import plot_batch, plot_img\n",
    "from src.datasets.render_tless_dataset import *\n",
    "from src.datasets.concat_dataset import ConcatDataset\n",
    "from src.ycb_render.tless_renderer_tensor import *\n",
    "from src.config.config import cfg, cfg_from_file, get_output_dir, write_selected_class_file\n",
    "\n",
    "\n",
    "class DummyFile(object):\n",
    "    def write(self, x): pass\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def nostdout():\n",
    "    save_stdout = sys.stdout\n",
    "    sys.stdout = DummyFile()\n",
    "    yield\n",
    "    sys.stdout = save_stdout\n",
    "    \n",
    "@gin.configurable\n",
    "def load_ycbrenderer_dataset(dis_dir:     str=gin.REQUIRED,\n",
    "                             cfg_file:    str=gin.REQUIRED,\n",
    "                             model_path:  str=gin.REQUIRED,):\n",
    "\n",
    "    cfg_from_file(cfg_file)\n",
    "    cfg.MODE = 'TRAIN'\n",
    "    print(cfg.TRAIN.OBJECTS)\n",
    "    print(cfg.TRAIN.RENDER_SZ)\n",
    "    print(cfg.TRAIN.INPUT_IM_SIZE)\n",
    "\n",
    "    # set up render\n",
    "    models = cfg.TRAIN.OBJECTS[:]\n",
    "    # with nostdout():\n",
    "    renderer = TLessTensorRenderer(cfg.TRAIN.RENDER_SZ, cfg.TRAIN.RENDER_SZ)\n",
    "    if cfg.TRAIN.USE_OCCLUSION:\n",
    "        with open('./src/datasets/tless_classes.txt', 'r') as class_name_file:\n",
    "            class_names_all = class_name_file.read().split('\\n')\n",
    "            for class_name in class_names_all:\n",
    "                if class_name not in models:\n",
    "                    models.append(class_name)\n",
    "\n",
    "        class_colors_all = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255),\n",
    "                            (128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 0), (128, 0, 128), (0, 128, 128),\n",
    "                            (64, 0, 0), (0, 64, 0), (0, 0, 64), (64, 64, 0), (64, 0, 64), (0, 64, 64),\n",
    "                            (155, 0, 0), (0, 155, 0), (0, 0, 155), (155, 155, 0), (155, 0, 155), (0, 155, 155),\n",
    "                            (200, 0, 0), (0, 200, 0), (0, 0, 200), (200, 200, 0),\n",
    "                            (200, 0, 200), (0, 200, 200)\n",
    "                            ]\n",
    "\n",
    "\n",
    "        obj_paths = ['{}/tless_models/{}.ply'.format(model_path, item) for item in models]\n",
    "        texture_paths = ['' for cls in models]\n",
    "        renderer.load_objects(obj_paths, texture_paths, class_colors_all)\n",
    "        renderer.set_projection_matrix(cfg.TRAIN.RENDER_SZ, cfg.TRAIN.RENDER_SZ, cfg.TRAIN.FU, cfg.TRAIN.FV,\n",
    "                                       cfg.TRAIN.RENDER_SZ/2.0, cfg.TRAIN.RENDER_SZ/2.0, 0.01, 10)\n",
    "        renderer.set_camera_default()\n",
    "        renderer.set_light_pos([0, 0, 0])\n",
    "\n",
    "    # synthetic dataset\n",
    "    dataset_train = tless_multi_render_dataset(model_path, cfg.TRAIN.OBJECTS, renderer,\n",
    "                                             render_size=cfg.TRAIN.RENDER_SZ,\n",
    "                                             output_size=cfg.TRAIN.INPUT_IM_SIZE)\n",
    "\n",
    "    # background dataset\n",
    "    dataset_dis = DistractorDataset(dis_dir, cfg.TRAIN.CHM_RAND_LEVEL,\n",
    "                                    size_crop=(cfg.TRAIN.INPUT_IM_SIZE[1],\n",
    "                                               cfg.TRAIN.INPUT_IM_SIZE[0]))\n",
    "    \n",
    "    return dataset_train, dataset_dis\n",
    "\n",
    "class InfiniteIter:\n",
    "    \"\"\"\n",
    "    Custom iterator that infinitely loops over a single provided data sample\n",
    "    \"\"\"\n",
    "    def __init__(self, data, bs=64):\n",
    "        self.__dict__.update(vars())\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        data = []\n",
    "        for d in self.data:\n",
    "            if isinstance(d, tuple):\n",
    "                d = d[0]\n",
    "            data.append(torch.stack([d] * self.bs))\n",
    "        return data\n",
    "    \n",
    "\n",
    "@gin.configurable\n",
    "def train_aae(num_workers: int=gin.REQUIRED,\n",
    "              num_train_iters: int=gin.REQUIRED,\n",
    "              cache_save_interval: int=gin.REQUIRED,\n",
    "              batch_size: int=gin.REQUIRED,\n",
    "              device: str=gin.REQUIRED):\n",
    "    \n",
    "    num_workers = 0\n",
    "    batch_size = 64\n",
    "    \n",
    "    with nostdout():\n",
    "        dataset_train, dataset_dis = load_ycbrenderer_dataset()\n",
    "    dl = DataLoader(dataset_train,\n",
    "                   batch_size=batch_size,\n",
    "                   shuffle=True,\n",
    "                   num_workers=num_workers)\n",
    "\n",
    "    bg_dl = DataLoader(dataset_dis,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=True,\n",
    "                       num_workers=num_workers)\n",
    "\n",
    "\n",
    "    def produce_augmented_data(data, bg, device):\n",
    "        im, target, _, mask, _, _, _, _, _, roi_affine, *_= data\n",
    "        \n",
    "        im         = im.to(device)\n",
    "        target     = target.to(device)\n",
    "        mask       = mask.to(device)\n",
    "        bg         = bg.to(device)\n",
    "        roi_affine = roi_affine.to(device)\n",
    "        \n",
    "        grids  = F.affine_grid(roi_affine, im.size())\n",
    "        images = F.grid_sample(im, grids)\n",
    "        mask   = F.grid_sample(mask, grids)\n",
    "        mask   = 1 - mask\n",
    "        \n",
    "        imdisp = im + mask * bg\n",
    "        noise_level = np.random.uniform(0, 0.05)\n",
    "        imdisp += torch.randn_like(imdisp) * noise_level\n",
    "        \n",
    "        return (imdisp, target)\n",
    "    \n",
    "    \n",
    "    for data, bg in tqdm(zip(dl, bg_dl), desc=\"AAE Training\"):\n",
    "        data = produce_augmented_data(data, bg, device)\n",
    "        break\n",
    "    \n",
    "        \n",
    "        \n",
    "    imdisp, target = data\n",
    "    single_data_sample = (imdisp[0], target[0], torch.zeros(imdisp[0].shape))\n",
    "    infinite_dl = InfiniteIter(single_data_sample, batch_size)\n",
    "    data = next(infinite_dl)\n",
    "    \n",
    "    model = AugmentedAutoEncoder(fixed_batch=data, log_to_wandb=False)\n",
    "    model = model.to(device)\n",
    "    batch_iters = 5000\n",
    "    num_iters = 4\n",
    "    cache_save_interval = 2\n",
    "\n",
    "    ######################################\n",
    "    # Optimization Step \n",
    "    ######################################\n",
    "    for epoch in tqdm(range(num_train_iters), desc=\"AAE Training\"):\n",
    "\n",
    "        is_save_epoch = ((epoch + 1) % cache_save_interval == 0)\n",
    "\n",
    "        [model.optimize_params(data, device=device, cache_recon=is_save_epoch) for _ in tqdm(range(batch_iters), desc=f\"Epoch: {epoch + 1}\", leave=False)]\n",
    "\n",
    "        if is_save_epoch:\n",
    "            model.save_state(epoch)\n",
    "            \n",
    "            \n",
    "        model.log(epoch, is_save_epoch)\n",
    "        \n",
    "        if is_save_epoch:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    return model\n",
    "    \n",
    "        \n",
    "gin.enter_interactive_mode()\n",
    "gin.parse_config_file('./config/train/linemod/[test]_obj_0001.gin')\n",
    "\n",
    "\n",
    "model = train_aae()\n",
    "\n",
    "print(f\"Visualization of Models Predictions (Image Order: Input, Target, Reconstruction)\\n{'=' * 100}\")\n",
    "model_result_reel = model._comp_log[-1]['Training/Random_Reconstruction_Visualizations']\n",
    "if isinstance(model_result_reel, torch.Tensor):\n",
    "    plot_img(T.to_pil_image(model_result_reel), fig_size=(30, 30))\n",
    "else:\n",
    "    print(f\"Visualizations were plotted to wandb please see them there\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
